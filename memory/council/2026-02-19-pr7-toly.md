# Council Review: toly — PR #7
_Reviewed on 2026-02-19 06:28 UTC_

# Council Review: PR #7 — State Recovery in common.py

Alright, let's break this down. Error recovery for missing state — straightforward defensive programming. But I'm seeing some things that need clarification before we ship.

## Throughput Analysis

**Good news:** This is a read-path optimization. Load errors happen at startup or on-demand, not in the hot loop. No blocking introduced. If `load_state()` is being called per-request in a high-frequency codepath, we need to know that — but assuming it's initialization or periodic, this is clean.

**Question:** How often does this fallback get hit in production? If it's frequent, we have a bigger problem than error recovery — we have a data reliability issue upstream. This PR patches a symptom.

## Parallelism Audit

State loading is inherently sequential — you need the state before you proceed. No parallel wins here. Not a blocker, just an observation. The try/except doesn't introduce serialization overhead. ✓

## Crypto-Native Angle

Here's where I want to push back slightly:

**Graceful degradation can hide problems.** In a distributed system, if state vanishes or corrupts, that's a *signal*. Starting fresh with an empty default might mask:
- Validator state divergence
- Byzantine behavior (intentional corruption)
- Disk failures that should trigger alerts
- Reorg inconsistencies

**The logging is good** — stderr warning means operators can see it. But "logging and continuing" needs teeth:
- Does this get surfaced to monitoring?
- Is there a circuit breaker if corruption repeats?
- Can we cryptographically verify state integrity (hash, signature)?

In Solana, if a validator's state is corrupt, we don't just reset and hope. We validate against the network. What's our equivalent here?

## Questions Before Approval

1. **What does "empty default state" actually contain?** Need to see that struct. An agent starting with `{}` vs. a proper zero-state matters.

2. **Frequency telemetry:** How many times per day is this fallback triggered in production? If >0, we need root cause analysis first.

3. **Verifiability:** Is there any hash or signature check on state.json? Or are we trusting the filesystem blindly?

4. **Recovery path:** After this fallback, does the agent re-sync state from peers/canonical source? Or does it just... continue with empty state?

5. **Test coverage:** Are there tests for the corruption case? Actual JSON corruption, not just missing file?

## The Real Issue

This PR is titled "error recovery" but it's really "error suppression." Recovery implies *getting back to a known good state*. Suppression means *forgetting what went wrong and starting over*.

For agents, that might be fine if state is ephemeral and re-derivable. But the PR doesn't clarify that. If state is critical to correctness, this is dangerous.

## Toly's Call

The code itself — try/except, fallback, logging — is solid engineering. But it's incomplete. Before merging, we need:

- Explicit documentation of what the fallback state contains and why it's safe
- Monitoring integration so ops sees these failures
- A root cause fix plan (why is state corrupting in the first place?)
- Verification that agents can recover or re-sync from a blank state

Ship it as-is? **No.** It works, but we're papering over a reliability issue without understanding it.

**Fix:** Add the telemetry + verification layer. Then we're golden.

VOTE: REVISE
